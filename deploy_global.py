# -*- coding: utf-8 -*-
"""deploy_global.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jjJg46tqkeAgWsdI5-WXi7CAp3DeRoKc
"""

import subprocess

# Install dependencies from requirements.txt
subprocess.check_call(['pip', 'install', '-r', 'requirements.txt'])

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("Cleaned_World_Development_Data.csv")

# Convert percentage strings to numeric values where applicable
def clean_percentage(column):
    if column.dtypes == 'object':  # Only process object (string) columns
        try:
            return column.str.replace('%', '').astype(float) / 100
        except:
            return column  # Return the column as is if conversion fails
    return column  # Return numeric columns as is

# Apply the cleaning function selectively
df_cleaned = df.copy()  # Create a copy to avoid modifying the original DataFrame
df_cleaned = df_cleaned.apply(clean_percentage)
# Check for non-numeric columns after cleaning
non_numeric_columns = df_cleaned.select_dtypes(include=['object']).columns
print("Non-numeric Columns:", non_numeric_columns)

# Drop non-numeric columns for correlation analysis
df_numeric = df_cleaned.drop(columns=non_numeric_columns)
# Separate numeric and non-numeric columns
numeric_columns = df_cleaned.select_dtypes(include=['number']).columns
non_numeric_columns = df_cleaned.select_dtypes(exclude=['number']).columns
# Fill missing values for numeric columns with their mean
df_cleaned[numeric_columns] = df_cleaned[numeric_columns].fillna(df_cleaned[numeric_columns].mean())
# Fill missing values for non-numeric columns with a placeholder or mode
df_cleaned[non_numeric_columns] = df_cleaned[non_numeric_columns].fillna("Unknown")

# Create numeric_df by selecting numeric columns from df
numeric_df = df_cleaned[numeric_columns]

# Create nu_df DataFrame with selected columns from numeric_df
nu_df = numeric_df[['GDP', 'CO2 Emissions', 'Energy Usage', 'Internet Usage']]

from sklearn.preprocessing import StandardScaler
# Standardization
scaler = StandardScaler()

# Now we can use nu_df for scaling
scaled_df = scaler.fit_transform(nu_df)

#K-Means Clustering
# Finding the optimal number of clusters using Elbow Method
import numpy as np
import matplotlib.pyplot as plt
import sklearn
from sklearn.cluster import KMeans

inertia = []
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    # Use the correct variable name 'scaled_df' here
    kmeans.fit(scaled_df)
    inertia.append(kmeans.inertia_)

plt.figure(figsize=(8, 5))
plt.plot(range(1, 11), inertia, marker='o')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')
plt.show()

# Final K-Means Model
optimal_clusters = 2 # Based on the Elbow plot
kmeans_model = KMeans(n_clusters=optimal_clusters, random_state=42)
kmeans_labels = kmeans_model.fit_predict(scaled_df)
import numpy as np

# Get the centroids of the clusters
centroids = kmeans_model.cluster_centers_

# Print the centroids for interpretation
print("Cluster Centroids:")
print(centroids)

if centroids[0][0] > centroids[1][0] and centroids[0][3] > centroids[1][3]:
    cluster_labels = {0: "Developed", 1: "Underdeveloped"}
else:
    cluster_labels = {0: "Underdeveloped", 1: "Developed"}

#saving the model
import joblib

joblib.dump(kmeans_model, 'kmeans_model.joblib')

joblib.dump(scaler, 'scaler.joblib')

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import joblib
# import pandas as pd
# import numpy as np
# 
# # Load the trained KMeans model and scaler
# kmeans_model = joblib.load('kmeans_model.joblib')
# scaler = joblib.load('scaler.joblib')  # Load the pre-trained scaler
# 
# st.title('Country Development Classification')
# 
# # Input fields in the Streamlit app
# GDP = st.number_input("Enter GDP", min_value=0.0, value=0.0)
# CO2_Emissions = st.number_input("Enter CO2 Emissions", min_value=0.0, value=0.0)
# Energy_Usage = st.number_input("Enter Energy Usage", min_value=0.0, value=0.0)
# Internet_Usage = st.number_input("Enter Internet Usage", min_value=0.0, value=0.0)
# 
# # Create a dictionary for user input
# input_data = {
#     'GDP': GDP,
#     'CO2 Emissions': CO2_Emissions,
#     'Energy Usage': Energy_Usage,
#     'Internet Usage': Internet_Usage
# }
# 
# # Convert input data to DataFrame
# input_df = pd.DataFrame([input_data])
# 
# # Cluster labels mapping
# cluster_labels = {0: "Underdeveloped", 1: "Developed"}
# 
# # Validate and process input
# if input_df.isnull().values.any():
#     st.error("Please fill in all the required fields.")
# else:
#     try:
#         # Standardize input data using the loaded scaler
#         scaled_input_data = scaler.transform(input_df)
# 
#         # Predict the cluster
#         cluster_id = kmeans_model.predict(scaled_input_data)[0]
# 
#         # Get the human-readable label
#         cluster_label = cluster_labels[cluster_id]
# 
#         # Display the result
#         st.success(f'The country is classified as: {cluster_label}')
#     except Exception as e:
#         st.error(f"An error occurred: {e}")
#

